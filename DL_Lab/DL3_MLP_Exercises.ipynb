{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzvwiv0lUfxi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1mnXsWUfnC0"
   },
   "source": [
    "In this notebook we will create our model to classify the FashionMNIST dataset. We will implement a Multilayer Perceptron (MLP) with one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP6zLCxZZuPr"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL_pNOTkjJY2"
   },
   "source": [
    "## Small introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRw2oKyzZ70t"
   },
   "source": [
    "In this lab we will implement the MLP from scratch. Recall how an input $X \\in \\mathbb{R}^{n×d}$ with $n$ samples and $d$ dimensions moves forward in the network. The output from the first layer is\n",
    "\n",
    "- $H = \\sigma(XW_1 + b_1)$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d×h}$ are the weights of the first layers and $b_1 \\in \\mathbb{R}^{1×h}$ are the bias. We consider $h$ hidden units. Finally, $\\sigma$ is the activation function.\n",
    "\n",
    "Since we will consider just one hidden layer, the output layer is given as\n",
    "\n",
    "- $O = H W_2 + b_2$\n",
    "\n",
    "where $H \\in \\mathbb{R}^{n×h}$, $W_2 \\in \\mathbb{R}^{h×q}$ and $b_2 \\in \\mathbb{R}^{1×q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5DpeD4_jPhj"
   },
   "source": [
    "## Building the model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCwgXCGnbNS9"
   },
   "source": [
    "To build our network we will always inherit from PyTorch `nn.Module` as this takes care of many problems for us. In the case of our MLP, the hyperparameters are the number of inputs, the number of outputs and the number of hidden units. For now just look at this class and make sure you understand it, do not worry about the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpRgXNNnfDkW"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLPInit(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr = 0.01, sigma=0.01):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving the hyperparameters\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.lr = lr\n",
    "\n",
    "        \n",
    "    # for now you should ignore this\n",
    "    def forward(self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HghWV8JYhb9d"
   },
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZfpZWPZeyR1"
   },
   "source": [
    "First, we will need to initialize the parameters for our MLP. For that we will use the `nn.Parameter` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZfEPvfhe7hw"
   },
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "W = nn.Parameter(torch.randn(5, 3) * sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeZ5tMUVfQXf"
   },
   "source": [
    "You will notice that `W` works like a usual Tensor. However, one of the advantages is that when used in the `nn.Module` class it will be recognized as a parameter of our model. Besides, it is always created with `requires_grad=True` so we do not need to worry about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1693784211779,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "F4Q6NePYfP2Q",
    "outputId": "ee9872af-aa38-460c-fcf4-c349f69b536a"
   },
   "outputs": [],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqef1DyIfbFf"
   },
   "source": [
    "Now let's initialize all of the parameters in our `MLPInit` class. We have to initialize $W_1, W_2, b_1, b_2$. The weights should be initialized with random values from a Gaussian distribution of zero mean and sigma variance. The bias should be initialized with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZ5ezYB-bb4h"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLPInit(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr = 0.01, sigma=0.01):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving the hyperparameters\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.lr = lr\n",
    "\n",
    "\n",
    "      # Exercise - initialize the parameters with nn.Parameter class and the correct dimensions\n",
    "\n",
    "\n",
    "    # for now you should ignore this\n",
    "    def forward(self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04OuuW-tf7-b"
   },
   "source": [
    "Now we will create a model with 4 input features, 3 outputs and 5 hidden units. Make sure your code passes all asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2p_4qT6gR7T"
   },
   "outputs": [],
   "source": [
    "model = MLPInit(num_inputs=4, num_outputs=3, num_hiddens=5)\n",
    "assert model.W1.shape == (4,5)\n",
    "assert model.b1.shape == (5,)\n",
    "assert model.W2.shape == (5,3)\n",
    "assert model.b2.shape == (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op_06ty7hSRV"
   },
   "source": [
    "Loop over `model.parameters()` printing the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1693784306648,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "IbIasLWFf7jz",
    "outputId": "44b05430-f4aa-4786-a12a-d8320de43923"
   },
   "outputs": [],
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMNke-TbhfeF"
   },
   "source": [
    "## Forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKKssiuch13J"
   },
   "source": [
    "Now that we have the parameters we are ready to implement our forward method, but first we need to define an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYpXWZqii5dQ"
   },
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oaaB6FTi7ii"
   },
   "source": [
    "For the activation function $\\sigma()$ we will use the ReLU function. Recall that\n",
    "\n",
    "$ReLU(x) = \\textrm{max}(x,0)$\n",
    "\n",
    "Implement the ReLU function and plot it in the interval [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1693784315855,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "-NMcaMzZi04W",
    "outputId": "83989140-57c8-4ba2-954f-e69f92ccc042"
   },
   "outputs": [],
   "source": [
    "# Exercise\n",
    "def relu(X):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbBrXOPjlWC-"
   },
   "source": [
    "We will not rely on our implementation but we will use `torch.nn.functional` module. Run the cell below, you should get the same plot as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1693784318139,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "Iw5E2dnPkgW8",
    "outputId": "444bc3c6-dd18-4d1a-afee-6ec27fc80252"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.arange(-5.0, 5.0, 0.1)\n",
    "y = F.relu(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNlEJpVYjhoI"
   },
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSalDIPtll1B"
   },
   "source": [
    "Now that we have defined our activation function and know how to implement it, it is time to finally complete our `forward` method.  Remember that we want to take an input $X$ and pass it through all the layers of the network (in the correct order). You should implement\n",
    "\n",
    "- $H = ReLU(XW_1 + b_1)$\n",
    "- $O = H W_2 + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfSAiz-Rl3Ou"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLPScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr=0.01, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.lr = lr\n",
    "\n",
    "        # Exercise - Just copy here your parameter initialization from before\n",
    "\n",
    "\n",
    "    # Exercis - implement the forward method\n",
    "    def forward(self, X):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX4iv5M6m_Wo"
   },
   "source": [
    "When you inherit from `nn.Module` you must define a `forward` method, which is why we always defined it above. Now let's test your implementation with a small batch $X$ of 8 samples and 4 features. When we run `model(X)` the forward method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1693784336040,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "2Ljul7NdmhTD",
    "outputId": "ba5fd243-9514-47eb-fab6-b155958e3973"
   },
   "outputs": [],
   "source": [
    "model = MLPScratch(num_inputs=4, num_outputs=3, num_hiddens=5)\n",
    "X = torch.randn(8, 4)\n",
    "output = model(X)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc0t_05Lr3bP"
   },
   "source": [
    "What is the meaning of this output? As expected the shape of the output is the number of samples (8) by the number of outputs (3). In classification, the number of outputs corresponds to the number of possible classes that we are trying to classify from. And consequently, the values in `output` tell us how likely each sample is to belong to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h6xEjXvsUTD"
   },
   "source": [
    "However, these are not probabilities as you can easily check. For this, we  will use the soft-max function that will squish each input between 0 and 1 and then normalize all the values, so that they sum to 1.\n",
    "\n",
    "$\\textrm{softmax}(x_i) = \\frac{\\textrm{exp}(x_i)}{\\sum_j \\textrm{exp}(x_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UD2bOYcpqA0h"
   },
   "source": [
    "We can use `F.softmax` to implement this. Compute the softmax for the output above and check that the values are now probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1693784374069,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "tYwQRhudsrdv",
    "outputId": "e1985b79-db3d-4eb9-c725-84b934d517ca"
   },
   "outputs": [],
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5yAs4Lyonz7"
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFAo1d3st1jg"
   },
   "source": [
    "We have our model but it is still not trained. I.e, we have yet to find values for W and b (remember that they were initialized with random values or zeros). To find the weights and bias, we will need to minimize a loss function. In a classification problem where we are using the softmax function as an output, we will want to use the cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bpl3Dtgwt11"
   },
   "source": [
    "$l(y,o) = -\\sum_j^q y_j \\log \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amtxJq_xon4R"
   },
   "source": [
    "Naturally, PyTorch also provides us with loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsYTe59NwcHa"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qU72naoxZ8o"
   },
   "source": [
    "An important thing to take into account is that  `CrossEntropyLoss()` already computes the softmax, so you should not give as input `probs` but rather `output`. The true labels for our batch are $[0,1,2,2,0,2,1,2]$. Compute the current cross entropy loss between the `output` and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1693784400692,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "hAO7Nvnwon4R",
    "outputId": "11e25d44-2bc3-4427-c994-ce2db00a04cc"
   },
   "outputs": [],
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53z84o14h3CJ"
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSKKhbEhyPz-"
   },
   "source": [
    "Our goal is now to minimize this loss function. For that we will use gradient descent. In a neural network this is done through backpropagation, where we propagate the gradient of the loss over the network (backwards) using the chain rule. Finally, recall that our update to the weights will be given as\n",
    "\n",
    "$W' = W - \\alpha \\nabla_W L$\n",
    "\n",
    "where $W$ is the current parameter value and W' is the new value, $\\alpha$ is the learning rate and $\\nabla_W L$ the gradient of our loss with respect to W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2zLabOpwtuG"
   },
   "source": [
    "## Optimizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgHQfvXE1oZn"
   },
   "source": [
    "Let us start with a simple example by taking a loss function $f = 2x^Tu$, where both $x$ and $u$ are parameters. Create a tensor $x = [0,1]$ and $u = [2,3]$ and create f. Look at the values of x, u and their gradients (remember to set `requires_grad`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1693784434295,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "qrB-vaLIOLL2",
    "outputId": "eb31413e-eda4-42d3-8e59-b5296a2693e6"
   },
   "outputs": [],
   "source": [
    "# Exercise\n",
    "def myloss(x,u):\n",
    "  # implement the loss\n",
    "\n",
    "\n",
    "# create x and u\n",
    "\n",
    "\n",
    "l = myloss(x,u)\n",
    "print(x,u)\n",
    "print(x.grad, u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RIkNpeHzKkt"
   },
   "source": [
    "Now run the follwing cell multiple times. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1693784446048,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "WiQbwkFozMce",
    "outputId": "a00b9539-44f7-4b9f-d757-27bc7a62f98d"
   },
   "outputs": [],
   "source": [
    "l = myloss(x,u)\n",
    "l.backward()\n",
    "print(x,u)\n",
    "print(x.grad, u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77K34BTlzgNK"
   },
   "source": [
    "Why is the gradient changing if we are not changing x or u? When we do several backward passes with the same paramters, gradients will be accumulated, not replaced. At each pass of our training loop we will want to have the current gradient, not the accumulation with previous passes. So we will need to take care of this. Fortunately, we can easily set gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693784447655,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "uRZtu5eB1-VD",
    "outputId": "822ec27f-ae87-420e-b8a7-ea9448b5576d"
   },
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqDU-5R0R1"
   },
   "source": [
    "Now, what will be the values of x and u after one optimization step with learning rate of 0.5? Try to do it by hand as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfKg8nBiSIF2"
   },
   "outputs": [],
   "source": [
    "x = torch.arange(2.0, requires_grad=True)\n",
    "u = torch.arange(2.0, 4.0, requires_grad=True)\n",
    "l = myloss(x,u)\n",
    "\n",
    "# Exercise compute the value of x and u after an update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL0iVjnLWcDZ"
   },
   "source": [
    "We are now ready to build our own Stochastic Gradient Descent Optimizer. You should complete the two methods, `step` and `zero_grad`. We need the `zero_grad` method to set all gradients to zero and prevent them from accumulating as we have seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGgr9fvkWi6m"
   },
   "outputs": [],
   "source": [
    "class MySGD():\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params # list of parameters to be updated\n",
    "        self.lr = lr # learning rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad(): #  disable gradient calculation while updating parameters\n",
    "          # Exercise update all parameters in self.params (just like you did above)\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # Exercise set each of the grad of the parameters to zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbbQtQEAXEtX"
   },
   "source": [
    "Let us test our optimizer with the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693784451436,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "zM6tmi_kXEDC",
    "outputId": "4dbd6555-22f7-4d95-925c-49caa2efd166"
   },
   "outputs": [],
   "source": [
    "# Just as before\n",
    "x = torch.arange(2.0, requires_grad=True)\n",
    "u = torch.arange(2.0, 4.0, requires_grad=True)\n",
    "l = myloss(x,u)\n",
    "\n",
    "# Test our optimizer\n",
    "optimizer = MySGD(params = [x,u], lr = 0.5)\n",
    "l.backward()\n",
    "optimizer.step()\n",
    "print(x,u)\n",
    "optimizer.zero_grad()\n",
    "print(x.grad,u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D679kH4IZEf0"
   },
   "source": [
    "## PyTorch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNO3Y7O5Yx__"
   },
   "source": [
    "Fortunately PyTorch already has an implemented optimizer class that we can use. It works in the same way. Replicate the results with `torch.optim.SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1693784455149,
     "user": {
      "displayName": "Filipa Valdeira",
      "userId": "09399709566484677368"
     },
     "user_tz": -60
    },
    "id": "5jwigWcEOv94",
    "outputId": "84ea043c-0885-4529-bd25-21a38b457399"
   },
   "outputs": [],
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KQFTf2Sd3jJ"
   },
   "source": [
    "By default `zero_grad()` sets the gradients to None and not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Id6Arhv2o_h"
   },
   "source": [
    "We now have all the elements to train our model and we will do so in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhIAq8vmOU_e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0CMp8L/nM+2RwQN+vS4Cf",
   "provenance": [
    {
     "file_id": "18JCI7NJQ8apUKGs8-evyonlywMYECxB_",
     "timestamp": 1693784466790
    },
    {
     "file_id": "1fxlvsNuYLOv6K4sqVzTGXHpB_I1R0bBU",
     "timestamp": 1693144873463
    },
    {
     "file_id": "1FPQAIs5f3OgaWy1TZ7ILnCKWf3kES-NK",
     "timestamp": 1693142496119
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
