{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a5be9",
   "metadata": {},
   "source": [
    "# Exercise 1 - Regression\n",
    "\n",
    "An important goal in this exercise is to show how sampling and noise in data can have a large effect on the hypotheses we find to model the data. This is especially important when dealing with small data sets.\n",
    "\n",
    "Load the dataset from the *polydata.npy* file. You can easily load this data using the *load()* function from the Numpy library.\n",
    "\n",
    "Split it into three sets: half the points for training, 30% for validation and 20% for test. The training set will be used to fit each polynomial curve. The validation set will be used to estimate the validation error of each polynomial curve and thus select the best one. The test set will be used to obtain an unbiased estimate of the true error of selected curve since selecting curves based on the error measure will make that measure biased. As we are working under the data sampling independence assumption, remember to shue the data randomly before splitting it because the order of the data in the file may not be random. Remember to standardize the data.\n",
    "\n",
    "Now select the best polynomial curve from polynomials of degree 1 through 15. Your script should report the best degree for the polynomial curve and also the test error of the best hypothesis. Also, do the plot of the dierent polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62cb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a109d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed818683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd142e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b15e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff93847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ac174d",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Exercise 2 - Logistic Regression\n",
    "\n",
    "### Exercise 2.1 - Model selection with cross-validation\n",
    "\n",
    "Load the dataset from the *ex2_data.txt* file. You can easily load this data using the *loadtxt()* function from the Numpy library. You can use the *t3_aux.py* file with auxiliary functions for this exercise ( *poly_16features(X)* and\n",
    "*create_plot* ).\n",
    "\n",
    "Load the data, split it into the training and test sets, and standardize it. Note that, in regression, as we did in the previous exercise, we also standardize the values to predict so that the results are more reproducible and to avoid numerical instability. In classification, however, the values to predict are class labels and it makes no sense to standardize those. So apply standardization only to the feature vectors. \n",
    "\n",
    "Then expand the data to 16 features. Use one third of the data for the test set. The training set will be used to select the number of features to use by cross-validation. Remember to shuffle the data randomly before splitting it because the order of the data in the file may not be random. Use stratified sampling for splitting the data and for\n",
    "the cross-validation folds.\n",
    "\n",
    "Now select the best logistic regression model by 10-fold cross-validation. Plot the training and cross-validation error for the different models to select the best one. If several models have very similar cross-validation errors, prefer a model with fewer features. \n",
    "\n",
    "Try running your code a few times to get a feeling for how the results vary depending on the assignment of points to training and testing sets. Note that this is often the case with small data sets, although the eect is less pronounced the more data there is available. Once you select the correct number of features, train a logistic regression with that number of features using the full training set and measure the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6adc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e46ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c5f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c0688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c21836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f9029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb548d5",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - Regularization\n",
    "\n",
    "Use cross validation to find the best C value for regularization of the 16-features model. Start with\n",
    "C = 1.0 and double the value of C at each iteration for 20 iterations, reporting the C value and the cross validation error for each iteration. Once you select the best value for C, train a logistic regression with that value using the full training set and measure the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683fce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea8aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "289b2b40",
   "metadata": {},
   "source": [
    "## Exercise 3 - Support Vector Machines\n",
    "\n",
    "In this exercise you will train a Support Vector Machine to classify the data set. Load the dataset from the *ex3_data.txt* file. You can easily load this data using the *loadtxt()* function from the Numpy library. You can use the *ex3_aux.py* file with auxiliary functions for this exercise, to plot the results.\n",
    "\n",
    "Load the data, standardize it and split it one half for training and one half for test. Keep\n",
    "the test set for the final exercise and use the training set to train SVM classifiers using the\n",
    "following kernels:\n",
    "\n",
    "- Polynomial, $K(x, y)=(x^T y + r)d$, with **degree 3**, **gamma of 0.5** and **r of 0** (the r value is set in the *coef0* parameter and is 0 by default).\n",
    "- Sigmoid, $K(x, y) = \\tanh(x^T y + r)$, with **gamma of 0.5** and **r of -2**.\n",
    "- Gaussian RBF, $K(x, y) = \\exp\\left(-\\gamma \\left\\| x - y \\right\\|^2\\right)$, with **gamma of 0.5**.\n",
    "\n",
    "Use the training set to optimize, with cross validation, the regularization parameter C for each of the three kernels above. Keep the remaining parameters constant, changing only the value of C, starting from 0.1 and doubling at each generation until greater than 10000. Plot the training and validation errors against the logarithm of C for each kernel and then train the SVM, for each kernel, with the best C. You can use the function *plot_svm(...)* in the *ex3_aux.py* file to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f35d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc810c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffa7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a4224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5deeaf4a",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9434f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6e929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c439f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f782fd04",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf30a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e71bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3864cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9acc07",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f8861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ef8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32799b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8c31b2",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## Exercise 4 - Vector Quantization with K-Means clustering\n",
    "\n",
    "Use the images *dying_star.jpg* and *birth_of_stars.jpg* to this exercise. In this exercise we are going to compute $k$ centroids to the 3D colour space with the K-Means clustering algorithm, and then convert each pixel into the closest centroid to compress the colour space reducing the number of different colours in the image. This is called vector quantization because we are converting vectors in a continuous space into a finite set of values (quantities).\n",
    "\n",
    "We start by loading the image using the *imread* function from the *skimage.io* module. We will also need the *imsave* function to save the images computed at the end. These functions simplify the conversion between image files and Numpy matrices. Note that the image is coded with one byte per colour, with an integer range of $0-255$. We will convert this into floating point values between $0-1$ by dividing the matrix by $255$.\n",
    "\n",
    "Reshape all the points into a matrix of colours with 3 columns. This is the matrix we will use for the *K-Means* quantization. Now use the *KMeans* class from the *sklearn.cluster* module to compute 64 centroids and then convert all the colours to the nearest centroid. You can get the centroid positions from the *cluster_centers_* attribute of your *K-Means* object and the labels from the *predict* method. To convert the colours you just need to assign the nearest centroid values to the pixel colour values. Finally, reshape the colours matrix into an image matrix and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0235e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_colour_space(data, colors, plot_title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(data[:,0], data[:,1], data[:,2], c=colors,s=10, alpha=0.7)\n",
    "    ax.set_xlabel('Red')\n",
    "    ax.set_ylabel('Green')\n",
    "    ax.set_zlabel('Blue')\n",
    "    ax.set_xlim3d(0,1)\n",
    "    ax.set_ylim3d(0,1)\n",
    "    ax.set_zlim3d(0,1)\n",
    "    ax.set_title(plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0a355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f945d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3daeac04",
   "metadata": {},
   "source": [
    "### K-Means with 64 centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f369a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca817679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b4f049",
   "metadata": {},
   "source": [
    "### K-Means with 8 centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daf2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d877e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad687198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe68d66b",
   "metadata": {},
   "source": [
    "# Additional exercise - Autoregressive (AR) Model for Orbit Prediction\n",
    "\n",
    "In this exercise, we will use an autoregression model to forecast the orbit of a satellite. Load the dataset from the *orbital_data.npy* file. You can easily load this data using the *load()* function from the Numpy library. This dataset contains 2500 samples with the timestamp of the data ($t$), the position of the object ($x$ and $y$) and the velocity of the object ($v_x$ and $v_y$) throughout the trajectory.\n",
    "\n",
    "In an autoregression model, we forecast our target values using a linear combination of its past values. The term autoregression indicates that it is a regression of the data against itself. An autoregressive model of order  \n",
    "$p$ can be written as\n",
    "\n",
    "\\begin{align}\n",
    "y_t = c + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\ldots + \\beta_p y_{t-p} + \\epsilon_t\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon_t$ denotes white noise. This is a regression problem but with lagged values of $y_t$. We refer to this as an **AR(p)** model, an autoregressive model of order p. *A more detailed explanation can be found in the book [Forecasting: Principles and Practice](https://otexts.com/fpp2/AR.html)*.\n",
    "\n",
    "## Exercise 5.1 - AR(p)\n",
    "\n",
    "In this exercise, we will just forecast the positions of the object throughout its trajectory. You can use the StatsModels library, which offers an implementation of an Autoregressive (AR) model. You should indicate the number $p$ of past values to consider at each timestep, the parameter *lags* (see documentation [here](https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html#statsmodels.tsa.ar_model.AutoReg)). Try different values for p (or *lags*) and see what results it produces. This library only accepts 1D series, so you have to use one model for each axis of the position, $x$ and $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04793e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ef061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61392540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0ea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e638140",
   "metadata": {},
   "source": [
    "## Exercise 5.2 - ARMA\n",
    "\n",
    "We can also consider past forecast errors in a regression model as\n",
    "\n",
    "\\begin{align}\n",
    "y_t = c + \\alpha_1 \\varepsilon_{t-1} + \\alpha_2 \\varepsilon_{t-2} + \\ldots + \\alpha_q \\varepsilon_{t-q} + \\epsilon_t\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon_t$ denotes white noise and $\\varepsilon_{k}$ denotes the forecast error at timestamp $k$. We refer to this as an **MA(q)** model, a moving average model of order q. Each value of $y_t$ can be thought of as a weighted moving average of the past $q$ forecast errors. *A more detailed explanation can be found in the book [Forecasting: Principles and Practice](https://otexts.com/fpp2/MA.html)*.\n",
    "\n",
    "We can combine both in a Autoregressive Moving Average model, **ARMA(p,q)**. You can use the StatsModels library, which offers an implementation of an ARIMA model. To adapt this model to just an **ARMA(p,q)** model, you should define the *order* parameter as $order=(p, 0, q)$. (see documentation [here](https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMA.html#statsmodels.tsa.arima.model.ARIMA)). Try different values for p and q. See what results it produces.\n",
    "\n",
    "*You can also try values different than zero for the second value of the $order$ parameter, however you will be using the AutoRegressive Integrated Moving Average model, (__ARIMA(p,d,q)__). If you have interest, a more detailed explanation can be found in the book [Forecasting: Principles and Practice](https://otexts.com/fpp2/non-seasonal-arima.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def5fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df4bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b82694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85078ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
